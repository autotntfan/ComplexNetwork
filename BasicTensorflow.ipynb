{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Layer\n",
    "import complexnn\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor: operate in CPU and GPU <br>\n",
    "ndarray: only in CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1,2],[3,4]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With different shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError:Can't convert non-rectangular Python sequence to Tensor.\n",
      "<tf.RaggedTensor [[0, 1, 2, 3], [3, 4, 1], [0]]>\n"
     ]
    }
   ],
   "source": [
    "element_list = [\n",
    "    [0, 1, 2, 3],\n",
    "    [3, 4, 1],\n",
    "    [0]\n",
    "]\n",
    "\n",
    "try:\n",
    "    tensor = tf.constant(element_list)\n",
    "except Exception as e:\n",
    "    print(f\"{type(e).__name__}:{e}\")\n",
    "    \n",
    "ragged_tensor = tf.ragged.constant(element_list)\n",
    "print(ragged_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6, shape=(), dtype=int32) tf.Tensor(3, shape=(), dtype=int32)\n",
      "[[3 4]\n",
      " [5 6]]\n",
      "tf.Tensor(\n",
      "[[0.26894143 0.7310586 ]\n",
      " [0.26894143 0.7310586 ]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor([3 4 5 6], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "maxvalue = tf.reduce_max(a)\n",
    "minvalue = tf.reduce_min(a)\n",
    "print(maxvalue,minvalue)\n",
    "maxvalueind = tf.argmax(a)\n",
    "# before using softmax, input argument should be in float type, otherwise an error arises with node failed.\n",
    "print(a)\n",
    "softvalue = tf.nn.softmax(tf.cast(a,tf.float32))\n",
    "print(softvalue)\n",
    "# flatten\n",
    "print(tf.reshape(a,[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)\n",
      "<dtype: 'float16'>\n"
     ]
    }
   ],
   "source": [
    "# array to tensor\n",
    "arr = np.array([[1,2],[3,4]])\n",
    "tensor = tf.constant(arr)\n",
    "print(tensor)\n",
    "# tensor to array\n",
    "arr = np.array(tensor)\n",
    "print(arr)\n",
    "# list to tensor\n",
    "lis = [1,2,3,4]\n",
    "tensor = tf.constant(lis)\n",
    "print(tensor)\n",
    "int_tensor = tf.constant([1,2])\n",
    "print(tf.cast(int_tensor,tf.float16).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 7), dtype=int32, numpy=\n",
       "array([[1, 1, 2, 3, 4, 4, 4],\n",
       "       [4, 4, 5, 6, 7, 7, 7],\n",
       "       [7, 7, 6, 1, 8, 8, 8]])>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = tf.constant([[1,2,3,4],\n",
    "                [4,5,6,7],\n",
    "                [7,6,1,8]])\n",
    "# repeats設定的參數中，要遵照axis設定的軸做重複，依照axis方向重複設定次數，而repeats的維度要符合輸入的維度\n",
    "# 若輸入為3*4，則axis設定為0時 repeats只能是三維，反之axis設定為1時repeats只能四維\n",
    "tf.repeat(g,repeats=[2,1,1,3],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  tf.Tensor(96.666664, shape=(), dtype=float32) y =  tf.Tensor([ 4.  7. 15.], shape=(3,), dtype=float32)\n",
      "tf.Tensor([ 2.6666667  4.666667  10.       ], shape=(3,), dtype=float32)\n",
      "tf.Tensor([ 2.6666667  9.333334  40.       ], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable([1.,2.,4.])\n",
    "b = tf.Variable([3.,3.,3.])\n",
    "x = tf.Variable([1.,2.,3.])\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = w*x + b\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "print('loss = ',loss,'y = ',y)\n",
    "# loss = y1^2+y2^2+y3^2 / 3 , dl/dy1 = 2*y1/3 ... and so on\n",
    "print(tape.gradient(loss,y))\n",
    "# dl/dx = dl/dy*dy/dx = dl/dy*[1 2 4]\n",
    "print(tape.gradient(loss,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用layer時，第一個index為數量，其次為長寬以及Channel數量 <br>\n",
    "因此若假設x為[[1.],[2.],[3.]]，shape of x is (3,1)，表有三組x，每組x只有一個值 <br>\n",
    "此時model dense unit = 2 則 y = w*x + b ，w為2*1矩陣，輸入1個x輸出2個值 <br>\n",
    "反之若x設定為[[1,2,3]]，shape of x is (1,3)，表示x只有一組，而每組x高度有三，因此實際上x是一個column vector，因此w會是3*2的矩陣\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_7/kernel:0' shape=(3, 2) dtype=float32, numpy=\n",
      "array([[-0.8785917 ,  0.88383484],\n",
      "       [-0.5125121 , -0.02549946],\n",
      "       [ 0.9924493 , -0.0401969 ]], dtype=float32)>, <tf.Variable 'dense_7/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]\n",
      "[<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
      "array([[1.0737319, 0.7122453],\n",
      "       [2.1474638, 1.4244906],\n",
      "       [3.2211957, 2.136736 ]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.0737319, 0.7122453], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "layer = tf.keras.layers.Dense(2, activation='relu')\n",
    "x = tf.constant([[1., 2., 3.]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  # Forward pass\n",
    "  y = layer(x)\n",
    "  loss = tf.reduce_mean(y**2)\n",
    "\n",
    "# Calculate gradients with respect to every trainable variable\n",
    "grad = tape.gradient(loss, layer.trainable_variables)\n",
    "# dl/dw = dl/dy*dy/dw = y*x =  X'@Y，dl/db = dl/dy\n",
    "print(layer.trainable_variables)\n",
    "print(grad)\n",
    "print('dl/dw = ',tf.transpose(x)@y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### watch a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n",
      "None\n",
      "['x0:0']\n",
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "['x0:0']\n"
     ]
    }
   ],
   "source": [
    "# A trainable variable\n",
    "x0 = tf.Variable(3.0, name='x0')\n",
    "# Not trainable\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
    "# Not a Variable: A variable + tensor returns a tensor.\n",
    "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
    "# Not a variable\n",
    "x3 = tf.constant(3.0, name='x3')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = (x0**2) + (x1**2) + (x2**2)\n",
    "\n",
    "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
    "\n",
    "for g in grad:\n",
    "    print(g)\n",
    "print([var.name for var in tape.watched_variables()])\n",
    "# add watch\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x2)\n",
    "    y = (x0**2) + (x1**2) + (x2**2)\n",
    "print(tape.gradient(y,x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3.]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "GradientTape.gradient can only be called once on non-persistent tapes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-af0b520e07cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Since GradientTape.gradient can only be called once, we need to set persistent tapes if we want to call it again.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\苯異丙胺\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m    978\u001b[0m     \"\"\"\n\u001b[0;32m    979\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 980\u001b[1;33m       raise RuntimeError(\"GradientTape.gradient can only be called once on \"\n\u001b[0m\u001b[0;32m    981\u001b[0m                          \"non-persistent tapes.\")\n\u001b[0;32m    982\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recording\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: GradientTape.gradient can only be called once on non-persistent tapes."
     ]
    }
   ],
   "source": [
    "x = tf.constant([2.,3.])\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "    r = x * z\n",
    "print(tape.gradient(r,z).numpy())\n",
    "# Since GradientTape.gradient can only be called once, we need to set persistent tapes if we want to call it again.\n",
    "print(tape.gradient(z,x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3.]\n",
      "[4. 6.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([2.,3.])\n",
    "with tf.GradientTape(persistent = True) as tape:\n",
    "    tape.watch(x)\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "    r = x * z\n",
    "print(tape.gradient(r,z).numpy())\n",
    "print(tape.gradient(y,x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用tape.gradient需注意變數是否為Variable，若已經變成Tensor，則無法使用，須加上watch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResourceVariable : tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.0>\n",
      "ResourceVariable : tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.0>\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = x*2\n",
    "\n",
    "    print(type(x).__name__, \":\", tape.gradient(y, x))\n",
    "    x.assign_add(1)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需特別注意使用decorator tf.function後， function裡不能使用Variable或創建Variable，但可以讀取function外的Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\r\n"
     ]
    }
   ],
   "source": [
    "# 二次微分\n",
    "@tf.function\n",
    "def seconder(x):\n",
    "    a = tf.constant(3.)\n",
    "    b = tf.constant(4.)\n",
    "    with tf.GradientTape() as tape2:        \n",
    "        with tf.GradientTape() as tape1:\n",
    "            y = x**3 + a*x**2 + b\n",
    "        dy_dx = tape1.gradient(y,x)\n",
    "    dy2_dx2 = tape2.gradient(dy_dx,x)\n",
    "    return dy2_dx2\n",
    "\n",
    "tf.print(seconder(tf.Variable(3.0,dtype=tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\r\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3,dtype=tf.float32)\n",
    "\n",
    "@tf.function\n",
    "def funwithvar():\n",
    "    a = tf.constant(3.)\n",
    "    b = tf.constant(4.)\n",
    "    with tf.GradientTape() as tape2:\n",
    "        with tf.GradientTape() as tape1:\n",
    "            y = x**3 + a*x**2 + b\n",
    "        dy_dx = tape1.gradient(y,x)\n",
    "    dy2_dx2 = tape2.gradient(dy_dx,x)\n",
    "    return dy2_dx2\n",
    "\n",
    "tf.print(funwithvar())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.999998569\r\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "x = tf.Variable(3.,dtype=tf.float32)\n",
    "f = lambda : x**2+2*x+1\n",
    "for _ in range(1000):\n",
    "    optimizer.minimize(f,x)\n",
    "tf.print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.99999851\r\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def minimizef():\n",
    "    for _ in tf.range(1000):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = x**2+2*x+1\n",
    "        dy_dx = tape.gradient(y,x)\n",
    "        optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])\n",
    "    return x\n",
    "tf.print(minimizef())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對於矩陣backpropagation而言，為了維持dA其shape=A，假設有三個矩陣A B C，C=AB，各自為A=(m,n) B=(n,p) C=(m,p)，則計算dC/dA時因為dC已知為(m,p)，dC/dA其shape應=A=(m,n)，值為與B相關，因此dC/dA=Ones@B'，其中為了使寬為n，需將B取transpose，而one的shape則=C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[14. 20.]\n",
      " [28. 36.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 62.  54. 156.]\n",
      " [120. 100. 292.]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[42. 42. 42.]\n",
      " [56. 56. 56.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([[1.0, 2.0, 3.0],\n",
    "                 [3.0, 4.0, 5.0],\n",
    "                 ], dtype=tf.float32)\n",
    "y = tf.Variable([[3.0,1.0],\n",
    "                 [1.0,2.0],\n",
    "                 [3.0,5.0]])\n",
    "z = tf.Variable([[3.0,1.0,4.0],\n",
    "                 [1.0,2.0,5.0],\n",
    "                 ])\n",
    "with tf.GradientTape() as tape:\n",
    "    x2 = x@y\n",
    "    print(x2)\n",
    "    x3 = x2@z\n",
    "    print(x3)\n",
    "    # This step is calculated with NumPy\n",
    "    # y = np.mean(x2, axis=0)\n",
    "\n",
    "\n",
    "    # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n",
    "    # using `tf.convert_to_tensor`.\n",
    "    #y = tf.reduce_mean(x2, axis=0)\n",
    "print(tape.gradient(x3, z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Python function.\n",
    "def a_regular_function(x, y, b):\n",
    "  x = tf.matmul(x, y)\n",
    "  x = x + b\n",
    "  return x\n",
    "\n",
    "# `a_function_that_uses_a_graph` is a TensorFlow `Function`.\n",
    "a_function_that_uses_a_graph = tf.function(a_regular_function)\n",
    "\n",
    "# Make some tensors.\n",
    "x1 = tf.constant([[1.0, 2.0]])\n",
    "y1 = tf.constant([[2.0], [3.0]])\n",
    "b1 = tf.constant(4.0)\n",
    "\n",
    "orig_value = a_regular_function(x1, y1, b1).numpy()\n",
    "# Call a `Function` like a Python function.\n",
    "tf_function_value = a_function_that_uses_a_graph(x1, y1, b1).numpy()\n",
    "assert(orig_value == tf_function_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=9>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def addfun(x):\n",
    "    return x**2\n",
    "g = tf.constant(3)\n",
    "addfun(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=35.0>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleModel(tf.Module):\n",
    "    def __init__(self,name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.a_variable = tf.Variable(5.,name='train_me')\n",
    "        self.non_train = tf.Variable(10.,trainable=False,name='donttrain')\n",
    "    def __call__(self,x):\n",
    "        return self.a_variable*x+self.non_train\n",
    "Simple = SimpleModel(name='simple')\n",
    "Simple(tf.constant(5.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NTD $1800.0"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Currency:\n",
    "    rates = {\n",
    "        'USD':1,\n",
    "        'NTD':30\n",
    "    }\n",
    "    def __init__(self, symbol,amount):\n",
    "        self.symbol = symbol\n",
    "        self.amount = amount\n",
    "    def __repr__(self):\n",
    "        return f'{self.symbol} ${self.amount}'\n",
    "    def convert(self, symbol):\n",
    "        new_amount = self.amount*self.rates[symbol]/self.rates[self.symbol]\n",
    "        return Currency(symbol, new_amount)\n",
    "    def __add__(self, other):\n",
    "        # 定義第一項為基準，將第一項的資訊代入self，第二項以後的代入other，因此other.convert使用的amount是other.amount，\n",
    "        # 將other.amount丟入convert後其值取代self.amount做運算\n",
    "        new_amount = self.amount + other.convert(self.symbol).amount\n",
    "        return Currency(self.symbol, new_amount)\n",
    "c1 = Currency('USD',10)\n",
    "c2 = Currency('NTD',600)\n",
    "c3 = Currency('NTD',900)\n",
    "c2+c1+c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results tf.Tensor([[1.9976461 0.3946366]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class Dense(tf.Module):\n",
    "    def __init__(self, input_, output_, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.w = tf.Variable(tf.random.normal([input_, output_], name='w'))\n",
    "        self.b = tf.Variable(tf.zeros([output_]),name='b')\n",
    "    def __call__(self, x):\n",
    "        y = tf.matmul(x, self.w) + self.b\n",
    "        return tf.nn.relu(y)\n",
    "class SequentialModule(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.dense_1 = Dense(input_=3, output_=3)\n",
    "        self.dense_2 = Dense(input_=3, output_=2)\n",
    "    @tf.function\n",
    "    def __call__(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        return self.dense_2(x)\n",
    "mymodel = SequentialModule(name='the_model')\n",
    "print('Model results',mymodel(tf.constant([[2.,2.,2.]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Variable 'b:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>, <tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[ 1.4399813 ,  0.48512435, -0.663558  ],\n",
      "       [ 2.2322783 , -1.419927  , -0.17691715],\n",
      "       [-1.76947   , -0.53400975, -0.9035559 ]], dtype=float32)>, <tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>, <tf.Variable 'Variable:0' shape=(3, 2) dtype=float32, numpy=\n",
      "array([[-0.2712436 , -0.41378766],\n",
      "       [-0.6923353 , -0.68764895],\n",
      "       [ 1.2357608 ,  0.20392382]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "print(mymodel.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.9117403  2.1225517  1.2693424 ]\n",
      " [2.793047   2.4954872  1.6266868 ]\n",
      " [0.9137399  1.2012966  0.12202454]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "img = np.array([[1.,2.,3.],\n",
    "                [1.,3.,4.],\n",
    "                [1.,1.,1.]],dtype=np.float32)\n",
    "layer1 = tf.keras.layers.Dense(3, activation='relu')\n",
    "y = layer1(img)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_13/kernel:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[ 0.03243327,  0.82836103, -0.23531985],\n",
      "       [ 0.7646129 , -0.17538404, -0.4326291 ],\n",
      "       [ 0.11669374,  0.5483196 ,  0.7899735 ]], dtype=float32)>, <tf.Variable 'dense_13/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(layer1.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense_17/Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
       "array([[1.9117403 , 2.1225517 , 1.2693424 ],\n",
       "       [2.793047  , 2.4954872 , 1.6266868 ],\n",
       "       [0.9137399 , 1.2012966 , 0.12202454]], dtype=float32)>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dense(tf.keras.layers.Layer):\n",
    "    def __init__(self, op, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.op = op\n",
    "    def build(self, ip):\n",
    "        self.w = tf.Variable([[ 0.03243327,  0.82836103, -0.23531985],\n",
    "       [ 0.7646129 , -0.17538404, -0.4326291 ],\n",
    "       [ 0.11669374,  0.5483196 ,  0.7899735 ]], name='w')\n",
    "        self.b = tf.Variable(tf.zeros([self.op], name='b'))\n",
    "    def call(self, ip):\n",
    "        return self.relu(tf.matmul(ip, self.w) + self.b)\n",
    "    def relu(self, x):\n",
    "        return tf.Variable(np.maximum(x,0))\n",
    "\n",
    "flxDense = Dense(op=3)\n",
    "flxDense(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-1.2871853  -0.7042976  -1.6350782  -0.50122947]\n",
      " [-1.2871853  -0.7042976  -1.6350782  -0.50122947]\n",
      " [-1.2871853  -0.7042976  -1.6350782  -0.50122947]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(2,activation='relu', name='layer1'),\n",
    "        layers.Dense(3,activation='relu', name='layer2'),\n",
    "        layers.Dense(4,name='layer3')\n",
    "    ]\n",
    ")\n",
    "x = tf.ones((3,3))\n",
    "y = model(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquivalentModel(tf.Module):\n",
    "    def __init__(self, outputsize=outputsize, name='name'):\n",
    "        super().__init__\n",
    "        self.outputsize = outputsize\n",
    "    def build(self, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.zeros((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.449389875 1.77287579 1.19182205]\n",
      " [0.560171068 0.387663901 0.509439468]\n",
      " [1.04390395 0.898369789 0.720921874]\n",
      " [0.63972795 0.687003791 0.642487049]\n",
      " [0.825822771 0.831765056 1.09128785]]\r\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "a = tf.random.normal([5,3],mean=1,stddev=0.5)\n",
    "tf.print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[0., 0., 0.],\n",
       "       [1., 2., 4.]], dtype=float32)>"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(x)\n",
    "x[1,:].assign(tf.Variable([1.,2.,4.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 3, 4), dtype=float32, numpy=\n",
       "array([[[[-1.0541916 , -1.0449066 ,  0.01452494,  1.0941601 ],\n",
       "         [ 0.44309902,  1.4887133 , -0.9086108 ,  0.4732647 ],\n",
       "         [-1.1899366 ,  1.4803028 , -0.03037643, -0.01228046]],\n",
       "\n",
       "        [[-1.8795247 ,  0.7430587 ,  0.526289  , -1.1686454 ],\n",
       "         [-0.24350691, -1.2441807 ,  1.0536628 , -0.10456944],\n",
       "         [-1.080358  , -0.7569585 ,  1.3990765 , -0.34365797]],\n",
       "\n",
       "        [[-1.173347  , -0.22561026,  0.2775097 ,  0.22138214],\n",
       "         [-0.2731552 , -0.6880598 , -0.80336905, -1.7259188 ],\n",
       "         [-0.02696133,  1.418993  ,  0.7238035 , -1.2891545 ]]],\n",
       "\n",
       "\n",
       "       [[[-1.9736538 , -0.8977804 , -1.7181797 , -0.16798544],\n",
       "         [ 0.32355738, -1.871625  ,  1.4709668 , -1.4523754 ],\n",
       "         [-0.76088   ,  1.9358897 , -0.44353962, -1.7894301 ]],\n",
       "\n",
       "        [[-1.1716471 ,  1.1550207 , -1.8895149 , -0.5326934 ],\n",
       "         [-1.9750752 ,  1.472372  ,  1.5591717 , -1.6778622 ],\n",
       "         [-1.158916  ,  0.2736063 , -0.20335054, -0.89886665]],\n",
       "\n",
       "        [[-0.70142984, -0.3175311 ,  1.483644  ,  1.0931883 ],\n",
       "         [ 0.2802801 , -0.7387347 , -0.54539776,  0.40279818],\n",
       "         [-0.81350756,  0.85534954,  1.0664377 , -0.43815374]]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.random.uniform([2,3,3,4],minval=-2,maxval=2)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-1.05419159 -1.04490662 0.0145249367 1.09416008]]\n",
      "\n",
      "  [[-1.87952471 0.743058681 0.526289 -1.16864538]]\n",
      "\n",
      "  [[-1.173347 -0.225610256 0.277509689 0.221382141]]]\n",
      "\n",
      "\n",
      " [[[-1.97365379 -0.897780418 -1.7181797 -0.167985439]]\n",
      "\n",
      "  [[-1.17164707 1.15502071 -1.88951492 -0.532693386]]\n",
      "\n",
      "  [[-0.701429844 -0.317531109 1.48364401 1.09318829]]]]\n",
      " \n",
      "[-1.05419159 1.48871326]\n",
      "[-1.05419159 -1.04490662 -0.908610821 ... -0.545397758 -0.813507557 -0.438153744]\n"
     ]
    }
   ],
   "source": [
    "tf.print(tf.gather(b,[0],axis=2))\n",
    "tf.print(' ')\n",
    "tf.print(tf.gather_nd(b,indices=[(0,0,0,0),(0,0,1,1)]))\n",
    "mask = tf.boolean_mask(b,b<0)\n",
    "tf.print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3 0.01]\n",
      " [-0 5 -0]]\n",
      " \n",
      "[[2 3 0.01]\n",
      " [0 5 0]]\n",
      "[[0 0 0]\n",
      " [0 3 0]\n",
      " [0 0 5]]\n",
      "[[2 3 0]\n",
      " [0 5 0]]\n"
     ]
    }
   ],
   "source": [
    "t = tf.Variable([[2,3,0.01],\n",
    "                 [-4,5,-5.4]])\n",
    "tf.print(t*tf.cast((t>0),tf.float32))\n",
    "tf.print(\" \")\n",
    "# tf.where(判斷式，若是則此，若否則此)\n",
    "# e.g. t pass through relu\n",
    "tf.print(tf.where(t>=0,t,0))\n",
    "# tf.scatter_nd(欲取代的indice,取代值，建立的矩陣大小)\n",
    "tf.print(tf.scatter_nd([[1,1],[2,2]],[3,5],(3,3)))\n",
    "# 抓值\n",
    "indices = tf.where(t>1)\n",
    "tf.print(tf.scatter_nd(indices,tf.gather_nd(t,indices),t.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([2, 3, 1, 2, 1])\n",
      "[[[[[17]\n",
      "    [172]]]\n",
      "\n",
      "\n",
      "  [[[236]\n",
      "    [89]]]\n",
      "\n",
      "\n",
      "  [[[31]\n",
      "    [189]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[40]\n",
      "    [123]]]\n",
      "\n",
      "\n",
      "  [[[220]\n",
      "    [52]]]\n",
      "\n",
      "\n",
      "  [[[197]\n",
      "    [72]]]]]\n",
      "TensorShape([2, 3, 2])\n",
      "TensorShape([2, 3, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.uniform(shape=[2,3,1,2,1],\n",
    "                      minval=0,maxval=255,dtype=tf.int32)\n",
    "tf.print(a.shape)\n",
    "tf.print(a)\n",
    "s = tf.squeeze(a,[2,4])\n",
    "tf.print(s.shape)\n",
    "r = tf.expand_dims(s,axis=3)\n",
    "tf.print(r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 5 6 9 10]\n",
      " [3 4 7 8 11 12]]\n",
      "[[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[5 6]\n",
      "  [7 8]]\n",
      "\n",
      " [[9 10]\n",
      "  [11 12]]]\n",
      "TensorShape([3, 2, 2])\n",
      "[[[[1 2]\n",
      "  [3 4]]], [[[5 6]\n",
      "  [7 8]]], [[[9 10]\n",
      "  [11 12]]]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1.0,2.0],[3.0,4.0]])\n",
    "b = tf.constant([[5.0,6.0],[7.0,8.0]])\n",
    "c = tf.constant([[9.0,10.0],[11.0,12.0]])\n",
    "# don't increase dimension\n",
    "tf.print(tf.concat([a,b,c],axis = 1))\n",
    "# increase dimension \n",
    "d = tf.stack([a,b,c],axis=0)\n",
    "tf.print(d)\n",
    "tf.print(d.shape)\n",
    "tf.print(tf.split(d,3,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=4.0>"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = tf.constant([1,2,3])\n",
    "d = tf.constant([[1],[2],[3]])\n",
    "# 個別計算c(1,3)和d(3,1)，取最大shape (3,3)\n",
    "tf.broadcast_dynamic_shape(tf.shape(c),tf.shape(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.477225575051661"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix norm (預設計算|A|2 即歐式距離)\n",
    "a = tf.constant([[1.0,2],[3,4]])\n",
    "tf.linalg.norm(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[-0.5228282 ,  0.95754106,  0.67781835],\n",
      "       [ 0.53549513, -0.97257476, -1.67468178],\n",
      "       [ 1.00576579,  1.69614907,  1.19309891]])\n",
      "array([[-0.5228282 ,  0.95754106,  0.67781835],\n",
      "       [ 0.53549513, -0.97257476, -1.67468178],\n",
      "       [ 1.00576579,  1.69614907,  1.19309891]])\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def np_random():\n",
    "    tf.print(np.random.randn(3,3))\n",
    "np_random()\n",
    "np_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2249216766696\r\n"
     ]
    }
   ],
   "source": [
    "aconstant = tf.constant(3)\n",
    "tf.print(id(aconstant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2250453860928\r\n"
     ]
    }
   ],
   "source": [
    "aconstant = aconstant + tf.constant(3)\n",
    "tf.print(id(aconstant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2248587534232\r\n"
     ]
    }
   ],
   "source": [
    "avar = tf.Variable(3)\n",
    "tf.print(id(avar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5 4.5\n",
      "end\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "tf.Tensor(8.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def myadd(a, b):\n",
    "    print(a,b)\n",
    "    for i in range(3):\n",
    "        c = a+b\n",
    "        tf.print(c)\n",
    "    print('end')\n",
    "    return c\n",
    "print(myadd(3.5,4.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "4.82201576e-05\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(0.,name='x',dtype=tf.float32)\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=.01)\n",
    "\n",
    "@tf.function\n",
    "def minimizef():\n",
    "    a = tf.constant(1.)\n",
    "    b = tf.constant(-2.)\n",
    "    c = tf.constant(1.)\n",
    "    \n",
    "    while tf.constant(True):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = a*x**2 + b*x + c\n",
    "        dy_dx = tape.gradient(y,x)\n",
    "        opt.apply_gradients(grads_and_vars=[(dy_dx,x)])\n",
    "        if tf.abs(dy_dx)<tf.constant(1e-4) or opt.iterations > 10000:\n",
    "            break\n",
    "        if opt.iterations%100 == 0:\n",
    "            tf.print(opt.iterations)\n",
    "    y = a*x**x+b*x+c\n",
    "    return y\n",
    "tf.print(minimizef())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-42-87c731a7a0b7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-42-87c731a7a0b7>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    for a,*kwargs in [(34,.5,9),name=1]:\u001b[0m\n\u001b[1;37m                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1 samples\n",
      "1/1 [==============================] - 2s 2s/sample - loss: 0.2756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18770572a20>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Mymodel(Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(name='w',\n",
    "                                 shape=(input_shape[-1],self.output_dim[1]))\n",
    "        self.b = tf.zeros(self.output_dim)\n",
    "    def call(self, input_data):\n",
    "        return tf.nn.relu(input_data@self.w + self.b)\n",
    "@tf.function\n",
    "def Myloss(y_true,y_pred):\n",
    "    dy = tf.abs(y_true-y_pred)**2\n",
    "    return tf.reduce_mean(dy)\n",
    "\n",
    "x_train = np.random.rand(1,4,4)\n",
    "y_train = np.random.rand(1,4,4)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Mymodel((4,4),input_shape=(4,4)))\n",
    "model.compile(loss=Myloss,optimizer='sgd')\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGOptimizer(tf.keras.optimizers.Optimizer):\n",
    "    def __init__(self, learning_rate=0.01, name='SGOptimizer',**kwargs):\n",
    "        super().__init__(name, **kwargs)\n",
    "        # kwargs.get('key',default_value)\n",
    "        self._set_hyper('learning_rate',kwargs.get('lr',learning_rate))\n",
    "        self._is_first = True\n",
    "    def _create_slots(self, var_list):\n",
    "        for var in var_list:\n",
    "            # add_slot(self,var,slot_name,initializer='zeros')\n",
    "            self.add_slot(var,'pv')\n",
    "        for var in var_list:\n",
    "            self.add_slot(var,'pg')\n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        var_dytpe = var.dtype.base_dtype\n",
    "        # _decayed_lr is a function where input is var_dtype and return a \"Tensor\" lr_t, where t denotes as tensor\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        \n",
    "        new_var_m = var - grad*lr_t\n",
    "        \n",
    "        pv_var = self.get_slot(var,'pv')\n",
    "        pg_var = self.get_slot(var,'pg')\n",
    "        \n",
    "        if self._is_first:\n",
    "            self._is_first = False\n",
    "            new_var = new_var_m\n",
    "        else:\n",
    "            cond = grad*pg_var >= 0\n",
    "            avg_weights = (pv_var+var)/2.\n",
    "            new_var = tf.where(cond,new_var_m,avg_weights)\n",
    "        pv_var.assign(var)\n",
    "        pg_var.assign(grad)\n",
    "        var.assign(new_var)\n",
    "    @tf.function\n",
    "    def get_config(self):\n",
    "        # baseconfig includes 'name','lr','decay','rho','momentum','epsilon','centered'.\n",
    "        # which are originally defined in the class optimizer\n",
    "        baseconfig = super().get_config()\n",
    "        return{\n",
    "            **base_config,\n",
    "            'learning_rate': self._serialize_hyperparameter('learning_rate')\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v\n",
      "g\n",
      "v\n",
      "g\n"
     ]
    }
   ],
   "source": [
    "def printvar(var):\n",
    "    for v in var:\n",
    "        print(v)\n",
    "    for v in var:\n",
    "        print(v)\n",
    "printvar(var=['v','g'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Could not interpret optimizer identifier:', <class '__main__.SGOptimizer'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-7024d2ba4fcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMyloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\苯異丙胺\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\苯異丙胺\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m             'Session arguments: %s' % (self._function_kwargs,))\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m     is_any_keras_optimizer_v1 = any(\n\u001b[0;32m    324\u001b[0m         (isinstance(opt, optimizers.Optimizer)\n",
      "\u001b[1;32mC:\\Users\\苯異丙胺\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_set_optimizer\u001b[1;34m(self, optimizer)\u001b[0m\n\u001b[0;32m   1391\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mopt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1392\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1393\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1395\u001b[0m     if (self._dtype_policy.loss_scale is not None and\n",
      "\u001b[1;32mC:\\Users\\苯異丙胺\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizers.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    850\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    851\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 852\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Could not interpret optimizer identifier:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: ('Could not interpret optimizer identifier:', <class '__main__.SGOptimizer'>)"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.RMSprop()\n",
    "m = Sequential()\n",
    "m.add(tf.keras.layers.Dense(10))\n",
    "m.compile(opt,loss=Myloss)\n",
    "data = np.arange(10).reshape(5,2)\n",
    "labels = np.zeros(5)\n",
    "m.fit(data,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, array([[4.3673746e-02, 1.1080883e-01, 6.9182432e-01, 1.5476604e+00,\n",
       "         3.3498514e+00, 3.6615089e-01, 6.4642442e-04, 2.9787531e-01,\n",
       "         3.8857648e-01, 1.8999034e+00],\n",
       "        [6.1185982e-02, 1.5152156e-01, 9.4002748e-01, 2.1276853e+00,\n",
       "         4.6072388e+00, 5.0921941e-01, 7.1580644e-04, 4.1476360e-01,\n",
       "         5.2664685e-01, 2.6182969e+00]], dtype=float32), array([1.4726669e-03, 3.1784929e-03, 1.8986195e-02, 4.6056870e-02,\n",
       "        9.9974766e-02, 1.1770521e-02, 1.7680625e-06, 9.6514719e-03,\n",
       "        1.0474568e-02, 5.7477832e-02], dtype=float32)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'sequential_2/dense/kernel:0' shape=(2, 10) dtype=float32, numpy=\n",
       " array([[ 0.0616572 , -0.16643564,  0.5910838 ,  0.41427416,  0.5858908 ,\n",
       "         -0.0273868 , -0.17450476,  0.04639716,  0.4931684 ,  0.35057634],\n",
       "        [-0.16498713, -0.03944265, -0.04282768,  0.34154063,  0.525469  ,\n",
       "          0.35929924,  0.14109105, -0.34209377, -0.07658245,  0.47198814]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'sequential_2/dense/bias:0' shape=(10,) dtype=float32, numpy=\n",
       " array([ 0.00316227,  0.00316227, -0.00316228, -0.00316228, -0.00316228,\n",
       "        -0.00316227,  0.00316204,  0.00316227, -0.00316227, -0.00316228],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'RMSprop',\n",
       " 'learning_rate': 0.001,\n",
       " 'decay': 0.0,\n",
       " 'rho': 0.9,\n",
       " 'momentum': 0.0,\n",
       " 'epsilon': 1e-07,\n",
       " 'centered': False}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.62896522 0.10233382 0.21579907 0.49109089]]\n",
      "\n",
      "  [[0.56970028 0.39445567 0.98970164 0.7154699 ]]\n",
      "\n",
      "  [[0.57889397 0.04475063 0.46639079 0.72181403]]]\n",
      "\n",
      "\n",
      " [[[0.01882047 0.04122241 0.39143425 0.94361904]]\n",
      "\n",
      "  [[0.699424   0.33666165 0.17879195 0.87850437]]\n",
      "\n",
      "  [[0.05198682 0.5983092  0.67545425 0.96231523]]]\n",
      "\n",
      "\n",
      " [[[0.25224298 0.55681762 0.2365431  0.07239631]]\n",
      "\n",
      "  [[0.67630574 0.2398951  0.96587005 0.51587215]]\n",
      "\n",
      "  [[0.51421852 0.3140129  0.7676805  0.80559188]]]]\n",
      ">>\n",
      "[[[[0.72505321 0.87619404 0.143162   0.36684588]]\n",
      "\n",
      "  [[0.58186199 0.35405305 0.3448931  0.88005846]]\n",
      "\n",
      "  [[0.35227904 0.8342426  0.53015419 0.08076486]]]\n",
      "\n",
      "\n",
      " [[[0.10828163 0.57195127 0.64915419 0.88776511]]\n",
      "\n",
      "  [[0.88123044 0.97559761 0.33725025 0.5872483 ]]\n",
      "\n",
      "  [[0.01035687 0.73842668 0.29159498 0.17183999]]]\n",
      "\n",
      "\n",
      " [[[0.84484815 0.69937252 0.01542592 0.15988129]]\n",
      "\n",
      "  [[0.71086461 0.78800423 0.04143069 0.34913379]]\n",
      "\n",
      "  [[0.17628222 0.26037583 0.11534248 0.71545696]]]]\n",
      ">>\n",
      "[[[[0.62896522 0.10233382 0.21579907 0.49109089]\n",
      "   [0.72505321 0.87619404 0.143162   0.36684588]]\n",
      "\n",
      "  [[0.56970028 0.39445567 0.98970164 0.7154699 ]\n",
      "   [0.58186199 0.35405305 0.3448931  0.88005846]]\n",
      "\n",
      "  [[0.57889397 0.04475063 0.46639079 0.72181403]\n",
      "   [0.35227904 0.8342426  0.53015419 0.08076486]]]\n",
      "\n",
      "\n",
      " [[[0.01882047 0.04122241 0.39143425 0.94361904]\n",
      "   [0.10828163 0.57195127 0.64915419 0.88776511]]\n",
      "\n",
      "  [[0.699424   0.33666165 0.17879195 0.87850437]\n",
      "   [0.88123044 0.97559761 0.33725025 0.5872483 ]]\n",
      "\n",
      "  [[0.05198682 0.5983092  0.67545425 0.96231523]\n",
      "   [0.01035687 0.73842668 0.29159498 0.17183999]]]\n",
      "\n",
      "\n",
      " [[[0.25224298 0.55681762 0.2365431  0.07239631]\n",
      "   [0.84484815 0.69937252 0.01542592 0.15988129]]\n",
      "\n",
      "  [[0.67630574 0.2398951  0.96587005 0.51587215]\n",
      "   [0.71086461 0.78800423 0.04143069 0.34913379]]\n",
      "\n",
      "  [[0.51421852 0.3140129  0.7676805  0.80559188]\n",
      "   [0.17628222 0.26037583 0.11534248 0.71545696]]]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.rand(3,3,1,4)\n",
    "b = np.random.rand(3,3,1,4)\n",
    "print(a)\n",
    "print('>>')\n",
    "print(b)\n",
    "print('>>')\n",
    "print(np.concatenate((a,b),axis=-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'complexnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-de18c9e3c40d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcomplexnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'complexnn'"
     ]
    }
   ],
   "source": [
    "import complexnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Complex)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
